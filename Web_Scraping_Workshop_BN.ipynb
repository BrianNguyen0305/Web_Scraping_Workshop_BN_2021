{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Web_Scraping_Workshop_BN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOUJZFUmm9qu+/HoKLhwLmU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrianNguyen0305/Web_Scraping_Workshop_BN_2021/blob/main/Web_Scraping_Workshop_BN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMMexB-A3jny"
      },
      "source": [
        "# **Web Scraping W/ Python**\n",
        "SPRING 2021 ANALYTICS WORKSHOP SERIES - TOPIC 2 <br/>\n",
        "UTDMSBA - BALC <br/>\n",
        "Brian Nguyen\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Agenda:\n",
        "\n",
        "\n",
        "1.   HTML Basics\n",
        "2.   Chrome DevTools\n",
        "3.   Compare Web-Scraping Packages \n",
        "4.   BeautifulSoup \n",
        " * Simple Exercise\n",
        "5.   Selenium \n",
        " * A-tad-harder Exercise\n",
        "6. Advanced Scraping and Crawling Demo\n",
        "7. Ethical & Efficiency Discussion\n",
        "\n",
        "\n",
        "## Goals:\n",
        "\n",
        "1. Inspect an HTML page & Identify what to scrape.\n",
        "2. Scrape with requests and BeautifulSoup.\n",
        "3. Drive web crawling with Selenium.\n",
        "4. How to be a responsible Scraper. \n",
        "\n",
        "## Why Scrape?\n",
        "1. Build Datasets:\n",
        "  * Texts\n",
        "  * Numbers\n",
        "  * Images\n",
        "2. For Analysis üìà\n",
        "  * Sales\n",
        "  * Marketing\n",
        "3. For Machine Learning ü§ñ\n",
        "4. End-to-end testing.\n",
        "5. Etc.\n",
        "\n",
        "---\n",
        "\n",
        "## I. HTML Basics\n",
        "\n",
        "### 1. Overview of HTML:\n",
        "1. Hypertext Markup Language (HTML)\n",
        "2. Standard markup language for documents.\n",
        "3. Instruct web browser how to display content. \n",
        "   * Provide structure.\n",
        "   * Cascading Style Sheets (CSS) = Style.\n",
        "   * JavaScript (or any script) = Interactive.\n",
        "4. Tags < > are the Elements. \n",
        "   *Paired\n",
        "   * Start: `<head>`\n",
        "   * End: `</head>`\n",
        "\n",
        "### 2. Common HTML Tags: \n",
        "1. `<!DOCTYPE html>` declaration defines this document to be HTML5.\n",
        "2. `<html>` element is the root element of an HTML page.\n",
        "3. `<div>` tag defines a division or a section in an HTML document. It's usually a container for other elements.\n",
        "4. `<head>` element contains meta information about the document.\n",
        "5. `<title>` element specifies a title for the document.\n",
        "6. `<body>` element contains the visible page content.\n",
        "7. `<h1>` element defines a large heading.\n",
        "8. `<p>` element defines a paragraph.\n",
        "9. `<a>` element defines a hyperlink. (look for `<href>`\n",
        "10. And Many More!\n",
        "\n",
        "### 3. Make a Simple HTML Page \n",
        "\n",
        "Notes:\n",
        "1. Note Repetitions\n",
        "2. Note Styles\n",
        "3. Note Buttons\n",
        "\n",
        "Tasks:\n",
        "1. Change Size for Heading 2, 3\n",
        "2. Fix Link 2, 3\n",
        "3. Fix Typo in Title (\"ZZZZZZ\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yoaix1io-OCB"
      },
      "source": [
        "from IPython.core.display import display, HTML"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv0Yyqhi-2ZV"
      },
      "source": [
        "display(HTML(\"\"\"<!doctype html>\n",
        "<html lang=\"en\">\n",
        "   <head>\n",
        "      <meta charset=\"utf-8\">\n",
        "      <meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shrink-to-fit=no\">\n",
        "      <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css\" \">\n",
        "      <title>My Courses</title>\n",
        "   </head>\n",
        "   <body>\n",
        "      <h1>Let's Get Scrapin' zzzzzzzzzzzzz !</h1>\n",
        "      <div class=\"card\" id=\"card-python-for-beginners\">\n",
        "         <div class=\"card-header\">\n",
        "            BeautifulSoup\n",
        "         </div>\n",
        "         <div class=\"card-body\">\n",
        "            <h3 class=\"card-title\">Web-Scraping for beginners</h5>\n",
        "            <p class=\"card-text\">If you are new to web-scraping, you should learn this!</p>\n",
        "              <p>Ordered list:</p>\n",
        "              <ol>\n",
        "                 <li>Data collection</li>\n",
        "                 <li>Exploratory data analysis</li>\n",
        "                 <li>Data analysis</li>\n",
        "                 <li>Policy recommendations</li>\n",
        "               </ol>\n",
        "  <hr>\n",
        "            <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\" class=\"btn btn-primary\" >Start for 20$</a>\n",
        "         </div>\n",
        "      </div>\n",
        "      <div class=\"card\" id=\"card-python-web-development\">\n",
        "         <div class=\"card-header\">\n",
        "            Selenium\n",
        "         </div>\n",
        "         <div class=\"card-body\">\n",
        "            <h4 class=\"card-title\">Web Automation with Selenium</h5>\n",
        "            <p class=\"card-text\">If you feel enough confident with XPath, you are ready to learn the most versatile web-scraping tool!</p>\n",
        "              <p style=\"color:red\">\n",
        "               Add colour to your paragraphs.\n",
        "               </p>\n",
        "            <a href=\"#\" class=\"btn btn-primary\">Start for 50$</a>\n",
        "         </div>\n",
        "      </div>\n",
        "        <div class=\"card\" id=\"card-python-machine-learning\">\n",
        "         <div class=\"card-header\">\n",
        "            Scrapy\n",
        "         </div>\n",
        "         <div class=\"card-body\">\n",
        "            <h5 class=\"card-title\">Master Web-Scraping with Scrapy</h5>\n",
        "            <p class=\"card-text\">Become a Web-Scraping master!</p>\n",
        "              <p>\n",
        "              That's a text paragraph. You can also <b>bold</b>, <mark>mark</mark>, <ins>underline</ins>, <del>strikethrough</del> and <i>emphasize</i> words.\n",
        "              You can also add links - here's one to <a href=\"https://en.wikipedia.org/wiki/Main_Page\">Wikipedia</a>.\n",
        "              </p>\n",
        "            <a href=\"#\" class=\"btn btn-primary\">Start for 100$</a>\n",
        "         </div>\n",
        "      </div>\n",
        "   </body>\n",
        "</html>\n",
        "\n",
        "\"\"\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adJm6kcLEnvs"
      },
      "source": [
        "### 4. ChromeDevTools\n",
        "\n",
        "Overview:\n",
        "1. Built in to Chrome.\n",
        "2. Super useful tool:\n",
        "   * View Source\n",
        "   * Inspect Elements\n",
        "   * Edit Webpage\n",
        "3. Equivalence available for other browsers. \n",
        "\n",
        "Quick Exercise: <br/>\n",
        "What is your favourite Website? <br/>\n",
        "a. IMDB <br/>\n",
        "b. Associated Press <br/>\n",
        "c. Reddit <br/>\n",
        "d. LinkedIn <br/>\n",
        "\n",
        "Tasks:\n",
        "1. Find Logo\n",
        "2. Find Text\n",
        "3. Find a Button \n",
        "\n",
        "Shortcuts:\n",
        "Command + Option + C (Mac) or Control + Shift + C (Windows) or F12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4e1mFfOGACg"
      },
      "source": [
        "## II. Web-Scraping W/ BeautifulSoup üç≤\n",
        "\n",
        "### Overview:\n",
        "\n",
        "1. Requests access, collect page source (all code).\n",
        "2. BeautifulSoup Is:\n",
        "   * Python Library.\n",
        "   * Extract HTML, XML files.\n",
        "   * Navigate and Scrape Webpage‚Äôs Tree structure.\n",
        "\n",
        "\n",
        "### Simple Exercise:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or3b7RijGldN"
      },
      "source": [
        "# Imports\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk0NvmYBGqT2"
      },
      "source": [
        "#¬†AP's homepage\n",
        "ap_url = 'https://apnews.com/'\n",
        "\n",
        "#¬†Use requests to retrieve data from a given URL\n",
        "ap_response = requests.get(ap_url)\n",
        "\n",
        "#¬†Parse the whole HTML page using BeautifulSoup\n",
        "# Second Argument is the parser method. Here let's try \"html.parser\" first\n",
        "ap_soup = BeautifulSoup(ap_response.text, 'html.parser')\n",
        "\n",
        "# Take a look at the mess that is ap_soup right now\n",
        "print(ap_soup.prettify()) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1UhiwqCH7SW",
        "outputId": "6fc644e5-c5b5-487e-d78f-82a964bf598b"
      },
      "source": [
        "#¬†Title of the parsed page\n",
        "ap_soup.title"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<title data-rh=\"true\">Associated Press News</title>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "scH0297ZIbEv",
        "outputId": "6214553e-a267-4f45-bc0d-1769faa70841"
      },
      "source": [
        "#¬†We can also get it without the HTML tags\n",
        "ap_soup.title.string"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Associated Press News'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp2L0vWXJoXp"
      },
      "source": [
        "### \"LXML\" Parser Method + For Loop\n",
        "Find() VS Find_all()\n",
        "\n",
        "We will use the `.find_all()` method to search the HTML tree for particular tags and get a `list` with all the relevant objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wF8g9TuoPlzJ"
      },
      "source": [
        "# Collect all code again but his time with the best parser method called 'lxml'\n",
        "ap_soup = BeautifulSoup(ap_response.text, 'lxml')\n"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMgMdYCUagmb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "IF2990ABJ_JX",
        "outputId": "fb340f91-931e-4873-90e2-f8e8f8933254"
      },
      "source": [
        "# Find top story (first one only)\n",
        "top = ap_soup.find('h1')\n",
        "\n",
        "# Show what we got\n",
        "top.text"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'AP source: Suspect in Capitol attack suffered delusions'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o8-cgwyXxbf"
      },
      "source": [
        "# Find all top stories \n",
        "top_all = ap_soup.find_all('h1')\n",
        "\n",
        "# Show what we got\n",
        "top_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ7KvzCkYHUy"
      },
      "source": [
        "for story in top_all:\n",
        "  title = story.text\n",
        "  print(title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzzWMBqdai18"
      },
      "source": [
        "## III. Web-Scraping W/ Selenium:\n",
        "\n",
        "### Overview:\n",
        "1. The most versatile of all web-scraper.\n",
        "2. In the right hand, it can become a Powerful Web Automator (Driver)\n",
        "3. Only one can read JavaScript easily.\n",
        "4. Can be very efficient when combined w/ Scrapy.\n",
        "5. IMO, Best Combo Right Now: Selenium + XPAth\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MURUfkccMW4"
      },
      "source": [
        "### \"A-tad-harder\" Exercise\n",
        "\n",
        "Colab is not the optimal environment for Selenium and Chromium driver <br/>\n",
        "Let's do a DEMO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8ihYduGa2xx"
      },
      "source": [
        "# Install Selenium\n",
        "%pip install selenium\n",
        "\n",
        "# Part of Scrapy, Parsel lets you extract data from XML/HTML documents using XPath or CSS selector\n",
        "%pip install parsel\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AGO_O6rbMVF"
      },
      "source": [
        "# Install Driver\n",
        "!apt update\n",
        "!apt install chromium-chromedriver"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-awL-9Ob0Ae"
      },
      "source": [
        "# Initiate and set options\n",
        "from parsel import Selector\n",
        "from selenium import webdriver\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "driver = webdriver.Chrome('chromedriver',options=options)"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bh5zXmRZb_j0",
        "outputId": "9f450552-4f34-42f1-8d25-bba2bed61b5f"
      },
      "source": [
        "# URL to use in Selenium\n",
        "driver.get('https://www.boxofficemojo.com/year/?ref_=bo_nb_di_secondarytab')\n",
        "\n",
        "# assigning the source code for the webpage to variable sel\n",
        "sel = Selector(text=driver.page_source)\n",
        "\n",
        "# xpath to extract the text from the class containing the movie title and year\n",
        "name = sel.xpath('//*[starts-with(@class, \"a-link-normal\")]/text()').getall()\n",
        "print(name)"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Total Gross', '%¬± LY', 'Releases', 'Average', 'Total Gross', '%¬± LY', 'Releases', 'Average', '2021', 'Tom and Jerry', '2020', 'Bad Boys for Life', '2019', 'Avengers: Endgame', '2018', 'Black Panther', '2017', 'Star Wars: Episode VIII - The Last Jedi', '2016', 'Finding Dory', '2015', 'Jurassic World', '2014', 'Guardians of the Galaxy', '2013', 'Iron Man 3', '2012', 'The Avengers', '2011', 'Harry Potter and the Deathly Hallows: Part 2', '2010', 'Avatar', '2009', 'Transformers: Revenge of the Fallen', '2008', 'The Dark Knight', '2007', 'Spider-Man 3', '2006', \"Pirates of the Caribbean: Dead Man's Chest\", '2005', 'Star Wars: Episode III - Revenge of the Sith', '2004', 'Shrek 2', '2003', 'Finding Nemo', '2002', 'Spider-Man', '2001', \"Harry Potter and the Sorcerer's Stone\", '2000', 'How the Grinch Stole Christmas', '1999', 'Star Wars: Episode I - The Phantom Menace', '1998', 'Titanic', '1997', 'Men in Black', '1996', 'Independence Day', '1995', 'Batman Forever', '1994', 'The Lion King', '1993', 'Jurassic Park', '1992', 'Batman Returns', '1991', 'Terminator 2: Judgment Day', '1990', 'Ghost', '1989', 'Batman', '1988', 'Who Framed Roger Rabbit', '1987', 'Beverly Hills Cop II', '1986', 'Top Gun', '1985', 'Back to the Future', '1984', 'Ghostbusters', '1983', 'Star Wars: Episode VI - Return of the Jedi', '1982', 'E.T. the Extra-Terrestrial', '1981', 'Superman II', '1980', 'Star Wars: Episode V - The Empire Strikes Back', '1979', 'Superman', '1978', 'Grease', '1977', 'Star Wars: Episode IV - A New Hope', 'News', 'Daily', 'Weekend', 'All Time', 'International', 'Showdowns', 'Glossary', 'User Guide', 'Help', 'IMDb', 'Conditions of Use', 'Privacy Policy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqDOYEtYdlac",
        "outputId": "04c5418f-f775-4ad8-9bf2-ecea75cb335c"
      },
      "source": [
        "# xpath to extract the text from the class containing the movie Total Gross\n",
        "gross = sel.xpath('//*[starts-with(@class, \"a-text-right mojo-field-type-money\")]/text()').getall()\n",
        "print(gross)"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['$244,468,683', '$2,222,442', '$2,085,651,481', '$4,593,945', '$11,320,874,529', '$12,426,865', '$11,889,341,443', '$11,973,153', '$11,072,815,067', '$12,996,261', '$11,377,066,920', '$13,290,966', '$11,125,835,068', '$13,151,105', '$10,359,575,749', '$12,202,091', '$10,922,051,943', '$13,222,823', '$10,822,806,722', '$13,411,160', '$10,173,621,826', '$13,936,468', '$10,566,830,616', '$16,231,690', '$10,590,200,693', '$16,393,499', '$9,629,131,592', '$13,281,560', '$9,657,106,911', '$12,460,783', '$9,208,611,128', '$12,343,982', '$8,837,713,363', '$13,073,540', '$9,365,047,036', '$13,378,638', '$9,210,978,005', '$13,809,562', '$9,165,532,414', '$16,079,881', '$8,110,859,106', '$19,638,884', '$7,511,547,085', '$17,110,585', '$7,377,967,100', '$16,468,676', '$6,725,527,166', '$20,136,308', '$6,156,263,535', '$19,858,914', '$5,647,751,531', '$18,456,704', '$5,199,428,915', '$17,867,453', '$5,101,025,737', '$19,695,080', '$4,860,902,708', '$18,205,628', '$4,556,151,332', '$18,445,956', '$4,366,922,856', '$17,260,564', '$4,362,843,546', '$18,486,625', '$4,111,404,298', '$17,495,337', '$3,548,516,187', '$14,847,348', '$3,398,925,607', '$15,039,493', '$3,102,793,651', '$15,436,784', '$3,041,480,248', '$15,923,980', '$3,104,296,629', '$18,368,619', '$2,748,432,836', '$18,445,857', '$3,008,355,492', '$22,790,571', '$918,310,755', '$16,398,406', '$1,657,166,297', '$24,370,092', '$1,244,961,893', '$31,124,047', '$840,508,376', '$64,654,490', '$443,497,478', '$49,277,497']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}